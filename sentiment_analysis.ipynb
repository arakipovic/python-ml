{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:03:03\n"
     ]
    }
   ],
   "source": [
    "#stolen from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch08/ch08.ipynb\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "basepath = '/Users/Users/arakipovic/Dropbox/ML'\n",
    "basepath = './aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('./movie_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Election is a Chinese mob movie, or triads in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was just watching a Forensic Files marathon ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Police Story is a stunning series of set piece...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Election is a Chinese mob movie, or triads in ...          1\n",
       "1  I was just watching a Forensic Files marathon ...          0\n",
       "2  Police Story is a stunning series of set piece...          1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./movie_data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'election': 66, 'is': 103, 'chinese': 44, 'mob': 128, 'movie': 132, 'or': 146, 'triads': 200, 'in': 98, 'this': 191, 'case': 40, 'every': 69, 'two': 205, 'years': 224, 'an': 11, 'held': 89, 'to': 196, 'decide': 54, 'on': 143, 'new': 137, 'leader': 115, 'and': 13, 'at': 20, 'first': 76, 'it': 104, 'seems': 170, 'toss': 198, 'up': 207, 'between': 32, 'big': 33, 'tony': 197, 'leung': 117, 'ka': 109, 'fai': 70, 'as': 18, 'know': 111, 'him': 93, 'the': 187, 'other': 147, 'lok': 120, 'simon': 175, 'yam': 222, 'who': 215, 'was': 209, 'judge': 107, 'full': 79, 'contact': 48, 'though': 192, 'once': 144, 'wins': 219, 'refuses': 162, 'accept': 4, 'choice': 45, 'goes': 81, 'whatever': 212, 'lengths': 116, 'he': 88, 'can': 39, 'secure': 169, 'recognition': 161, 'unlike': 206, 'any': 14, 'asian': 19, 'film': 75, 'watch': 210, 'featuring': 72, 'gangsters': 80, 'one': 145, 'not': 139, 'action': 6, 'has': 86, 'its': 105, 'bloody': 35, 'moments': 129, 'when': 213, 'necessary': 136, 'goodfellas': 82, 'but': 37, 'basically': 25, 'just': 108, 'really': 159, 'effective': 65, 'drama': 62, 'there': 188, 'are': 17, 'lot': 121, 'of': 141, 'characters': 43, 'which': 214, 'hard': 85, 'keep': 110, 'track': 199, 'think': 190, 'that': 186, 'plays': 153, 'into': 102, 'craziness': 52, 'all': 9, 'bit': 34, '100': 0, 'year': 223, 'old': 142, 'baton': 26, 'symbol': 184, 'power': 156, 'mentioned': 126, 'before': 29, 'changes': 41, 'hands': 84, 'several': 173, 'times': 195, 'things': 189, 'settle': 172, 'down': 61, 'may': 125, 'appear': 16, 'ends': 67, '65': 2, '70': 3, 'minute': 127, 'mark': 124, 'still': 178, 'couple': 50, 'surprises': 183, 'waiting': 208, 'my': 135, 'favorite': 71, 'character': 42, 'here': 91, 'sort': 177, 'anchors': 12, 'picture': 151, 'br': 36, 'quite': 157, 'award': 21, 'winner': 217, 'last': 113, 'hong': 95, 'kong': 112, 'awards': 22, 'winning': 218, 'for': 77, 'best': 31, 'actor': 7, 'director': 57, 'johnny': 106, 'did': 55, 'heroic': 92, 'trio': 201, 'screenplay': 167, 'also': 10, 'had': 83, 'nominations': 138, 'cinematography': 47, 'editing': 64, 'score': 166, 'loved': 122, 'three': 194, 'more': 130, 'acting': 5, 'performances': 150, 'including': 100, 'watching': 211, 'forensic': 78, 'files': 74, 'marathon': 123, 'court': 51, 'tv': 204, 'episode': 68, 'identical': 97, 'plot': 154, 'right': 163, 'incest': 99, 'secret': 168, 'affair': 8, 'with': 220, 'sister': 176, 'subplot': 182, 'don': 60, 'recall': 160, 'based': 24, 'true': 202, 'story': 179, 'disclaimer': 58, 'does': 59, 'have': 87, 'mow': 133, 'written': 221, 'over': 148, 'apparently': 15, 'chronicles': 46, 'real': 158, 'homicide': 94, 'ruby': 164, 'morris': 131, 'by': 38, 'her': 90, 'husband': 96, 'earl': 63, 'sentenced': 171, '25': 1, 'life': 118, 'murder': 134, 'show': 174, 'you': 225, 'truth': 203, 'be': 27, 'stranger': 180, 'than': 185, 'fiction': 73, 'because': 28, 'thought': 193, 'lifetime': 119, 'contrived': 49, 'stretch': 181, 'insofar': 101, 'believability': 30, 'posters': 155, 'said': 165, 'bad': 23, 'didn': 56, 'notice': 140, 'players': 152, 'lead': 114, 'daughter': 53, 'whose': 216, 'performance': 149}\n",
      "[[ 1  0  1  1  1  1  1  1  0  1  1  2  1  7  1  0  1  2  3  1  3  1  1  0\n",
      "   0  1  1  0  0  2  0  4  1  3  1  1  2  2  0  1  1  1  1  1  1  1  0  1\n",
      "   1  0  1  0  1  0  1  1  0  1  0  0  0  1  1  0  1  1  3  1  0  1  1  1\n",
      "   1  0  0  4  1  2  0  1  1  1  1  1  1  1  1  0  1  1  0  1  1  1  0  1\n",
      "   0  0  3  0  1  0  1  5  6  1  1  1  1  1  1  1  1  1  0  2  1  3  0  0\n",
      "   2  1  1  0  1  1  1  1  1  1  1  0  2  0  0  1  1  2  1  1  0  5  1  1\n",
      "   1  1  3  2  0  0  1  2  0  1  0  0  1  1  0  2  0  1  1  0  0  0  1  1\n",
      "   0  1  1  0  1  1  0  2  0  1  1  0  0  0  0  1  1  0  2  9  2  1  1  2\n",
      "   2  0  1  1  6  3  1  1  1  1  0  0  0  1  1  1  1  3  1  0  1  1  3  2\n",
      "   0  1  1  1  0  0  3  2  1  0]\n",
      " [ 0  1  0  0  0  1  0  0  1  2  0  0  0  2  1  1  0  0  1  0  0  0  0  2\n",
      "   1  0  0  1  1  0  1  0  0  0  0  0  0  1  1  1  1  0  1  0  0  0  1  0\n",
      "   0  1  0  1  0  1  0  0  1  0  1  1  1  1  0  1  0  0  0  0  1  0  0  0\n",
      "   0  1  1  0  0  1  1  0  0  2  0  0  0  0  0  1  0  0  2  0  0  0  1  0\n",
      "   1  1  0  1  0  1  0  0  4  0  0  0  2  0  0  0  0  0  1  0  0  0  1  1\n",
      "   0  0  0  1  0  0  0  0  0  0  1  1  1  1  1  0  0  0  0  0  1  3  0  2\n",
      "   0  0  0  1  1  1  0  0  1  0  2  1  0  0  1  1  1  0  0  1  1  1  0  0\n",
      "   1  0  0  1  0  0  1  0  1  0  0  1  1  1  1  0  0  2  0 13  0  0  0  1\n",
      "   1  1  0  0  5  0  0  0  0  0  1  1  1  0  0  0  0  6  0  1  0  0  0  1\n",
      "   1  0  0  0  3  1  0  0  1  1]]\n",
      "[[ 0.05  0.    0.05  0.05  0.05  0.03  0.05  0.05  0.    0.03  0.05  0.1\n",
      "   0.05  0.24  0.03  0.    0.05  0.1   0.1   0.05  0.14  0.05  0.05  0.    0.\n",
      "   0.05  0.05  0.    0.    0.1   0.    0.19  0.05  0.14  0.05  0.05  0.1\n",
      "   0.07  0.    0.03  0.03  0.05  0.03  0.05  0.05  0.05  0.    0.05  0.05\n",
      "   0.    0.05  0.    0.05  0.    0.05  0.05  0.    0.05  0.    0.    0.\n",
      "   0.03  0.05  0.    0.05  0.05  0.14  0.05  0.    0.05  0.05  0.05  0.05\n",
      "   0.    0.    0.19  0.05  0.07  0.    0.05  0.05  0.03  0.05  0.05  0.05\n",
      "   0.05  0.05  0.    0.05  0.05  0.    0.05  0.05  0.05  0.    0.05  0.    0.\n",
      "   0.14  0.    0.05  0.    0.05  0.24  0.2   0.05  0.05  0.05  0.03  0.05\n",
      "   0.05  0.05  0.05  0.05  0.    0.1   0.05  0.14  0.    0.    0.1   0.05\n",
      "   0.05  0.    0.05  0.05  0.05  0.05  0.05  0.05  0.03  0.    0.07  0.    0.\n",
      "   0.05  0.05  0.1   0.05  0.05  0.    0.17  0.05  0.03  0.05  0.05  0.14\n",
      "   0.07  0.    0.    0.05  0.1   0.    0.05  0.    0.    0.05  0.05  0.\n",
      "   0.07  0.    0.05  0.05  0.    0.    0.    0.05  0.05  0.    0.05  0.05\n",
      "   0.    0.05  0.05  0.    0.1   0.    0.05  0.05  0.    0.    0.    0.\n",
      "   0.05  0.05  0.    0.1   0.3   0.1   0.05  0.05  0.07  0.07  0.    0.05\n",
      "   0.05  0.2   0.14  0.05  0.05  0.05  0.05  0.    0.    0.    0.05  0.05\n",
      "   0.05  0.05  0.1   0.05  0.    0.05  0.05  0.14  0.07  0.    0.05  0.05\n",
      "   0.05  0.    0.    0.14  0.1   0.03  0.  ]\n",
      " [ 0.    0.07  0.    0.    0.    0.05  0.    0.    0.07  0.09  0.    0.    0.\n",
      "   0.09  0.05  0.07  0.    0.    0.05  0.    0.    0.    0.    0.13  0.07\n",
      "   0.    0.    0.07  0.07  0.    0.07  0.    0.    0.    0.    0.    0.\n",
      "   0.05  0.07  0.05  0.05  0.    0.05  0.    0.    0.    0.07  0.    0.\n",
      "   0.07  0.    0.07  0.    0.07  0.    0.    0.07  0.    0.07  0.07  0.07\n",
      "   0.05  0.    0.07  0.    0.    0.    0.    0.07  0.    0.    0.    0.\n",
      "   0.07  0.07  0.    0.    0.05  0.07  0.    0.    0.09  0.    0.    0.    0.\n",
      "   0.    0.07  0.    0.    0.13  0.    0.    0.    0.07  0.    0.07  0.07\n",
      "   0.    0.07  0.    0.07  0.    0.    0.19  0.    0.    0.    0.09  0.    0.\n",
      "   0.    0.    0.    0.07  0.    0.    0.    0.07  0.07  0.    0.    0.\n",
      "   0.07  0.    0.    0.    0.    0.    0.    0.05  0.07  0.05  0.07  0.07\n",
      "   0.    0.    0.    0.    0.    0.07  0.14  0.    0.09  0.    0.    0.\n",
      "   0.05  0.07  0.07  0.    0.    0.07  0.    0.13  0.07  0.    0.    0.07\n",
      "   0.05  0.07  0.    0.    0.07  0.07  0.07  0.    0.    0.07  0.    0.\n",
      "   0.07  0.    0.    0.07  0.    0.07  0.    0.    0.07  0.07  0.07  0.07\n",
      "   0.    0.    0.13  0.    0.61  0.    0.    0.    0.05  0.05  0.07  0.    0.\n",
      "   0.23  0.    0.    0.    0.    0.    0.07  0.07  0.07  0.    0.    0.    0.\n",
      "   0.28  0.    0.07  0.    0.    0.    0.05  0.07  0.    0.    0.    0.2\n",
      "   0.07  0.    0.    0.05  0.07]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#df = pd.read_csv('./movie_data.csv')\n",
    "count = CountVectorizer()\n",
    "docs = df['review'][0:2]\n",
    "\n",
    "bag = count.fit_transform(docs)\n",
    "print(count.vocabulary_)\n",
    "print(bag.toarray())\n",
    "\n",
    "tfdtf = TfidfTransformer()\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfdtf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^<]+?>', '', text)\n",
    "    emoticons = re.findall('(?::|;||=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ''.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/arakipovic/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 21.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', '...se_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'].values, df['sentiment'].values, test_size=0.1)\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None], \n",
    "               'vect__use_idf': [False],\n",
    "               'vect__norm': [None],\n",
    "               'clf__penalty': ['l1', 'l2'], \n",
    "               'clf__C': [1.0, 10.0, 100.0]}\n",
    "]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), \n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "gs = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__C': 1.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__norm': None, 'vect__stop_words': None, 'vect__use_idf': False}\n",
      "best score 0.8884\n",
      "test acc: 0.889\n"
     ]
    }
   ],
   "source": [
    "print('Best params: {}'.format(gs.best_params_))\n",
    "print('best score {}'.format(gs.best_score_))\n",
    "clf = gs.best_estimator_\n",
    "print('test acc: {}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc test: 0.8212\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [word for word in text.split()]\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer_porter_stop_remove(text):\n",
    "    return [porter.stem(word) for word in text.split() if word not in stop]\n",
    "\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label\n",
    "            \n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            doc, label = next(doc_stream)\n",
    "            docs.append(doc)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y\n",
    "\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', n_features=2**21, preprocessor=None, \n",
    "                         tokenizer=tokenizer_porter)\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "doc_stream = stream_docs(path='./movie_data.csv')\n",
    "\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "        \n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()\n",
    "    \n",
    "\n",
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('acc test: {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
